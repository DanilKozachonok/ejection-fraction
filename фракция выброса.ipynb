{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b0a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c880ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import * #импортируем все функции\n",
    "\n",
    "import os #функции для взаимодействия с операционной системой (например, с файлами)\n",
    "\n",
    "from tqdm import tqdm #Этот оператор импортирует класс tqdm из модуля tqdm.\n",
    "#tqdm - библиотека, которая предоставляет индикатор прогресса для итераций в циклах.\n",
    "#Он позволяет визуально отслеживать прогресс выполнения длительных операций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06f28f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e5eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce38d4",
   "metadata": {},
   "source": [
    "# Новый код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52ee52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "X_train shape: (5397, 8, 5000)\n",
      "y_train shape: (2, 5397)\n",
      "Processed shapes:\n",
      "X_train shape: torch.Size([5397, 8, 5000])\n",
      "Y_train shape: torch.Size([5397])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Функция для загрузки и предобработки данных\n",
    "def load_data():\n",
    "    # Загрузка данных из файлов .pkl\n",
    "    with open(\".\\\\..\\\\Data\\\\dumped\\\\X_train_fraction_train_1.pkl\", \"rb\") as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(\".\\\\..\\\\Data\\\\dumped\\\\y_train_fraction_train_1.pkl\", \"rb\") as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(\".\\\\..\\\\Data\\\\dumped\\\\X_test_fraction_test_1.pkl\", \"rb\") as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(\".\\\\..\\\\Data\\\\dumped\\\\y_test_fraction_test_1.pkl\", \"rb\") as f:\n",
    "        y_test = pickle.load(f)\n",
    "\n",
    "\n",
    "    print(\"Original shapes:\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "    # Приведение данных к типу NumPy\n",
    "    X_train = np.array(X_train, dtype=np.float32)  # Приводим к типу float32\n",
    "    X_test = np.array(X_test, dtype=np.float32)    # Приводим к типу float32\n",
    "\n",
    "    # Убедимся, что размеры верные и преобразуем в нужный формат\n",
    "    #X_train = X_train.reshape(-1, 8, 5000)  # Трансформируем в (кол-во образцов, 8, 5000)\n",
    "    #X_test = X_test.reshape(-1, 8, 5000)\n",
    "\n",
    "    # Преобразуем y_train и y_test в нужный формат\n",
    "    Y_train = np.array(y_train[0], dtype=np.int64)  # Приводим к типу int64\n",
    "    Y_test = np.array(y_test[0], dtype=np.int64)    # Приводим к типу int64\n",
    "\n",
    "    # Преобразуем данные в тензор\n",
    "    Y_train = torch.tensor(Y_train)\n",
    "    Y_test = torch.tensor(Y_test)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "# Загрузка данных\n",
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "\n",
    "# Проверка размерностей\n",
    "print(\"Processed shapes:\")\n",
    "print(\"X_train shape:\", X_train.shape)  # Ожидается (N, 8, 5000)\n",
    "print(\"Y_train shape:\", Y_train.shape)    # Ожидается (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f109ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([5398, 8, 5000])\n",
      "Y_test shape: torch.Size([5398])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape:\", X_test.shape)  # Ожидается (N, 8, 5000)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f91918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated X_train shape: torch.Size([5397, 3, 5000])\n"
     ]
    }
   ],
   "source": [
    "# Извлекаем данные из соответствующих каналов\n",
    "DI = X_train[:, 0, :]  # 1 канал\n",
    "DII = X_train[:, 1, :]  # 2 канал\n",
    "V1 = X_train[:, 2, :]  # 3 канал\n",
    "V2 = X_train[:, 3, :]  # 4 канал\n",
    "V3 = X_train[:, 4, :]  # 5 канал\n",
    "V4 = X_train[:, 5, :]  # 6 канал\n",
    "V5 = X_train[:, 6, :]  # 7 канал\n",
    "V6 = X_train[:, 7, :]  # 8 канал\n",
    "\n",
    "# Применение формул для вычисления x, y и z\n",
    "x = -(0.156 * DI - 0.01 * DII - 0.172 * V1 - 0.074 * V2 +\n",
    "      0.122 * V3 + 0.231 * V4 + 0.239 * V5 + 0.194 * V6)\n",
    "\n",
    "y = (- 0.227 * DI + 0.887 * DII + 0.057 * V1 - 0.019 * V2 -\n",
    "      0.106 * V3 - 0.022 * V4 + 0.041 * V5 + 0.048 * V6)\n",
    "\n",
    "z = -(0.022 * DI + 0.102 * DII - 0.229 * V1 - 0.31 * V2 -\n",
    "      0.246 * V3 - 0.063 * V4 + 0.055 * V5 + 0.108 * V6)\n",
    "\n",
    "# Добавление новых каналов в X_train\n",
    "X_train = torch.cat((X_train, x.unsqueeze(1), y.unsqueeze(1), z.unsqueeze(1)), dim=1)\n",
    "\n",
    "X_train = X_train[:, -3:, :]# Получаем последние три канала\n",
    "\n",
    "#X_train = X_train.permute(0, 2, 1)\n",
    "# Проверка размерностей\n",
    "print(\"Updated X_train shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d1c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "class PointCloudData(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        # Храним только последние три канала (x, y, z) из X\n",
    "        self.X = X\n",
    "        self.Y = Y  # Тензор меток классов (N, кол-во классов)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Подсчет образцов каждого класса\n",
    "        self.class_counts = self._count_classes()\n",
    "\n",
    "    def _count_classes(self):\n",
    "        # Используем Counter для подсчета меток классов\n",
    "        return dict(Counter(self.Y.numpy()))  # Преобразуем тензор меток в массив и считаем\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]  # Количество образцов\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Убедимся, что индекс не выходит за границы\n",
    "        if idx >= len(self.X):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset with size {len(self.X)}\")\n",
    "\n",
    "        point_cloud = self.X[idx]\n",
    "        target = int(self.Y[idx])  # массив меток классов\n",
    "\n",
    "        if self.transform:\n",
    "            point_cloud = self.transform(point_cloud.numpy())  # Применяем нормализацию\n",
    "\n",
    "        return {\n",
    "            'pointcloud': point_cloud,\n",
    "            'category': target,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e399f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    Normalize(),\n",
    "    PointSampler_weighted(5000),\n",
    "    RandomNoise(std=0.001),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4738db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PointCloudData object at 0x0000025C3EBEE890>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000025C3EBEDC60>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 4943, 1: 454}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаем объект датасета\n",
    "train_ds = PointCloudData(X_train, Y_train, transform =None)\n",
    "# Создаем загрузчик данных\n",
    "BATCH_SIZE = 64  # размер батча\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print(train_ds)\n",
    "print(train_loader)\n",
    "train_ds.class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb021d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1b17b",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeaad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число градиентных спусков за 1 эпоху равняется 84\n"
     ]
    }
   ],
   "source": [
    "# Определение количества итераций в одной эпохе\n",
    "iterations_per_epoch = len(train_loader)\n",
    "print('Число градиентных спусков за 1 эпоху равняется', iterations_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "360517f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointNet(\n",
       "  (transform): Transform(\n",
       "    (input_transform): Tnet(\n",
       "      (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (feature_transform): Tnet(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (logsoftmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pointnet = PointNet()\n",
    "pointnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a118858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c9bbb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/132252277748874109', creation_time=1722765439796, experiment_id='132252277748874109', last_update_time=1722765439796, lifecycle_stage='active', name='VECG_POINTNET_Classification', tags={}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация MLflow\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"VECG_POINTNET_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "297139e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([5397, 3, 5000])\n",
      "Y_train shape: torch.Size([5397])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbbb953",
   "metadata": {},
   "source": [
    "# Функции оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68fde5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointnetloss(outputs, labels, m3x3, m64x64, alpha=0.0001):\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    bs=outputs.size(0)\n",
    "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n",
    "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs,1,1)\n",
    "    if outputs.is_cuda:\n",
    "        id3x3=id3x3.cuda()\n",
    "        id64x64=id64x64.cuda()\n",
    "    diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n",
    "    diff64x64 = id64x64-torch.bmm(m64x64,m64x64.transpose(1,2))\n",
    "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3)+torch.norm(diff64x64)) / float(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f8c9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bfdcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def weighted_avg_f1(output, labels):\n",
    "    \"\"\"Функция расчета weighted avg F1-меры\"\"\"\n",
    "    # Преобразование списков в массивы numpy\n",
    "    output = np.array(output)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Создание тензоров PyTorch\n",
    "    output = torch.tensor(output)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    predictions = torch.argmax(output, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
    "    weighted_f1 = np.nan_to_num(weighted_f1, nan=0.0)  # Замена NaN на 0 при делении на 0\n",
    "\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b36eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output,labels):\n",
    "    \"\"\"Функция расчета accuracy\"\"\"\n",
    "    # Преобразование списков в массивы numpy\n",
    "    output = np.array(output)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Создание тензоров PyTorch\n",
    "    output = torch.tensor(output)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    predictions = torch.argmax(output,dim=1)\n",
    "    correct = (predictions == labels).sum().cpu().numpy()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8904cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Функция для логирования артефактов в MLflow\"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    " \n",
    "            outputs, _, _ = model(inputs) #inputs.transpose(1,2)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "    # Сохранение отчета в текстовый файл\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "    output_file = \"classification_report.txt\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(report)\n",
    "    mlflow.log_artifact(\"classification_report.txt\")\n",
    "    os.remove(\"classification_report.txt\")\n",
    "        \n",
    "    # Save confusion matrix as CSV artifact\n",
    "    df = pd.DataFrame(cm)\n",
    "    new_columns = [f\"predicted class {i}\" for i in range(df.shape[1])]\n",
    "\n",
    "    # Установка новых названий столбцов\n",
    "    df.columns = new_columns\n",
    "\n",
    "    # Добавление индексов\n",
    "    df.insert(0, \"real\\pred\", [f\"real class {i}\" for i in range(df.shape[0])])\n",
    "\n",
    "    # Сохранение измененной таблицы \n",
    "    df.to_csv(\"confusion_matrix.csv\", index=False)\n",
    "    mlflow.log_artifact(\"confusion_matrix.csv\")\n",
    "    os.remove(\"confusion_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81ed8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pointnet(model_pointnet, dataloader_train, batch_size, \n",
    "                     name_save, start_weight, \n",
    "                     name_experiment=None, lr=0.00025, epochs=8,\n",
    "                     scheduler=True, scheduler_step_size=10, dataset_name=None,\n",
    "                     f_sampling=700, seed=42, n_points=512,\n",
    "                     normalize='Centering and max value scaling', gamma=0.5, noise_std=0):\n",
    "    \"\"\"Обучение классификационной сети\n",
    "\n",
    "    Args:\n",
    "        model_pointnet: Класс модели pytorch\n",
    "\n",
    "        dataloader_train: Обучающий даталоудер\n",
    "\n",
    "        batch_size: Размер одного батча\n",
    "\n",
    "        name_save: Имя модели для сохранения в папку models\n",
    "\n",
    "        start_weight: Если указать веса, то сеть будет в режиме fine tune. Defaults to None.\n",
    "\n",
    "        name_experiment:  Имя эксперимента для MLflow. Нужно при mlflow_tracking=True. Defaults to None.\n",
    "        \n",
    "        lr: Скорость обучения. Defaults to lr=0.00025.\n",
    "\n",
    "        epochs: Число эпох обучения. Defaults to 100.\n",
    "\n",
    "        scheduler (bool): Включение/выключение lr шедулера. Defaults to True.\n",
    "\n",
    "        scheduler_step_size (int): Шаг шедулера при scheduler=True. Defaults to 10.\n",
    "\n",
    "        dataset_name: Имя датасета для логирования в MLflow. Defaults to None.\n",
    "\n",
    "        seed (int): Seed рандома. Defaults to 42.\n",
    "        \n",
    "        normalize: Вид нормализации\n",
    "\n",
    "        gamma: Величина коэффициента lr шедулера \n",
    "\n",
    "        noise_std: Величина std шума на трейне\n",
    "\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    if dataset_name != None:\n",
    "        dataset_name = dataset_name.split('\\\\')[-1]\n",
    "\n",
    "    directory_save = \".\\\\..\\\\models\"\n",
    "\n",
    "    if not os.path.exists(directory_save):\n",
    "        os.makedirs(directory_save)\n",
    "    \n",
    "    if name_experiment == None:\n",
    "        name_experiment = name_save\n",
    "    with mlflow.start_run(run_name=name_experiment) as run:\n",
    "        \n",
    "        model = model_pointnet()\n",
    "        mlflow.log_param(\"Model\", 'PointNet')\n",
    "        if start_weight != None:\n",
    "            model.load_state_dict(torch.load(start_weight, map_location=torch.device('cpu')))\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if scheduler:\n",
    "            gamma_val = gamma\n",
    "            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                                        step_size=scheduler_step_size,\n",
    "                                                        gamma=gamma_val)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        mlflow.log_param(\"Normalize\", normalize)\n",
    "        mlflow.log_param(\"Training random noise std\", noise_std)\n",
    "        mlflow.log_param(\"Input shape\", f'torch.Size([batch_size, {n_points}, 3])')\n",
    "        mlflow.log_param(\"F sampling ECG\", f_sampling)\n",
    "        mlflow.log_param(\"Points samping\", n_points)\n",
    "        if scheduler:\n",
    "            mlflow.log_param(\"scheduler\", 'On')\n",
    "            mlflow.log_param(\"scheduler_step_size\", scheduler_step_size)\n",
    "            mlflow.log_param(\"scheduler_gamma\", gamma_val)\n",
    "        else:\n",
    "            mlflow.log_param(\"scheduler\", 'Off')\n",
    "\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "        mlflow.log_param(\"optimizer\", 'Adam')\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"loss\", 'NLLLoss + 0.0001*Loss_reg')\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "        if start_weight != None:\n",
    "            mlflow.log_param(\"Fine-tuning\", True)\n",
    "        else:\n",
    "            mlflow.log_param(\"Fine-tuning\", False)\n",
    "\n",
    "        max_epoch_f1_train = 0\n",
    "        for epoch in range(epochs): \n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            all_outputs = []\n",
    "            all_targets = []\n",
    "            for i, data in enumerate(dataloader_train, 0):\n",
    "                inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Inputs shape: {inputs.shape}, Labels shape: {labels.shape}')\n",
    "                inputs = inputs.permute(0, 1, 2)\n",
    "                outputs, m3x3, m64x64 = model(inputs)\n",
    "                \n",
    "                loss = pointnetloss(outputs, labels, m3x3, m64x64)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                all_outputs.extend(outputs.cpu().detach().numpy())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "                \n",
    "            train_epoch_loss = running_loss / len(dataloader_train)\n",
    "            train_epoch_acc = accuracy(all_outputs, all_targets)\n",
    "            train_epoch_f1 = weighted_avg_f1(all_outputs, all_targets)\n",
    "\n",
    "            mlflow.log_metric(\"train_epoch_accuracy\", train_epoch_acc, step=(epoch+1))\n",
    "            mlflow.log_metric(\"train_epoch_loss\", train_epoch_loss, step=(epoch+1))\n",
    "            mlflow.log_metric(\"train_epoch_f1\", train_epoch_f1, step=(epoch+1))\n",
    "\n",
    "\n",
    "            if scheduler:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Вывод значения функции потерь на каждой 5 эпохе\n",
    "            #if ((epoch+1) % 5 == 0) or epoch==0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_epoch_loss:.4f},'\n",
    "                  f' Train Aсс: {train_epoch_acc:.4f}')\n",
    "\n",
    "\n",
    "            if epoch >= 1 and train_epoch_f1 > max_epoch_f1_train:\n",
    "                max_epoch_f1_train = train_epoch_f1\n",
    "                acc_model = train_epoch_acc\n",
    "                model_to_save = model\n",
    "                epoch_best = epoch + 1\n",
    "                name_save_model = directory_save + '\\\\' + name_save +'.pth'\n",
    "                torch.save(model_to_save.state_dict(), name_save_model)\n",
    "                evaluate_model(model=model, data_loader=dataloader_train)\n",
    "            \n",
    "        print('Обучение завершено')\n",
    "        print(f'Сохранена модель {name_save_model} с лучшим weighted avg f1 на train = {max_epoch_f1_train}')  \n",
    "        print('Accuracy данной модели равно', acc_model) \n",
    "\n",
    "        mlflow.log_metric(\"max f1 saved model\", max_epoch_f1_train)\n",
    "        mlflow.log_metric(\"accuracy of model\", acc_model)\n",
    "        mlflow.log_metric(\"epoch of save\", epoch_best)\n",
    "\n",
    "        mlflow.log_artifact(name_save_model)\n",
    "        #mlflow.log_artifact('model.py')\n",
    "\n",
    "\n",
    "\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3ad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 2, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 3, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 4, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 5, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 6, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 7, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 8, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 9, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 10, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 11, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 12, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 13, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 14, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 15, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 16, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n",
      "Epoch: 1, Batch: 17, Inputs shape: torch.Size([64, 3, 5000]), Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_pointnet(\n",
    "    model_pointnet=PointNet,\n",
    "    dataloader_train=train_loader,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    name_save='pointnet_weights_firstNetwork_FV',\n",
    "    start_weight= None,\n",
    "    name_experiment='train first Network_fv',\n",
    "    lr=0.00025,\n",
    "    epochs=8,\n",
    "    scheduler=42,\n",
    "    scheduler_step_size=10,\n",
    "    dataset_name=None,\n",
    "    f_sampling=700,\n",
    "    seed=42,\n",
    "    normalize='Centering and max value scaling',\n",
    "    gamma=0.5,\n",
    "    noise_std=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a80f2",
   "metadata": {},
   "source": [
    "# Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c8aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([5398, 8, 5000])\n",
      "Y_test shape: torch.Size([5398])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape:\", X_test.shape)  # Ожидается (N, 8, 5000)\n",
    "print(\"Y_test shape:\", Y_test.shape)    # Ожидается (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852ad728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated X_test shape: torch.Size([5398, 3, 5000])\n"
     ]
    }
   ],
   "source": [
    "# Извлекаем данные из соответствующих каналов\n",
    "DI = X_test[:, 0, :]  # 1 канал\n",
    "DII = X_test[:, 1, :]  # 2 канал\n",
    "V1 = X_test[:, 2, :]  # 3 канал\n",
    "V2 = X_test[:, 3, :]  # 4 канал\n",
    "V3 = X_test[:, 4, :]  # 5 канал\n",
    "V4 = X_test[:, 5, :]  # 6 канал\n",
    "V5 = X_test[:, 6, :]  # 7 канал\n",
    "V6 = X_test[:, 7, :]  # 8 канал\n",
    "\n",
    "# Применение формул для вычисления x, y и z\n",
    "x = -(0.156 * DI - 0.01 * DII - 0.172 * V1 - 0.074 * V2 +\n",
    "      0.122 * V3 + 0.231 * V4 + 0.239 * V5 + 0.194 * V6)\n",
    "\n",
    "y = (- 0.227 * DI + 0.887 * DII + 0.057 * V1 - 0.019 * V2 -\n",
    "      0.106 * V3 - 0.022 * V4 + 0.041 * V5 + 0.048 * V6)\n",
    "\n",
    "z = -(0.022 * DI + 0.102 * DII - 0.229 * V1 - 0.31 * V2 -\n",
    "      0.246 * V3 - 0.063 * V4 + 0.055 * V5 + 0.108 * V6)\n",
    "\n",
    "# Добавление новых каналов в X_test\n",
    "X_test = torch.cat((X_test, x.unsqueeze(1), y.unsqueeze(1), z.unsqueeze(1)), dim=1)\n",
    "\n",
    "X_test = X_test[:, -3:, :]# Получаем последние три канала\n",
    "\n",
    "#X_train = X_test.permute(0, 2, 1)\n",
    "# Проверка размерностей\n",
    "print(\"Updated X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "                    Normalize(),\n",
    "                    PointSampler_weighted(5000),\n",
    "                    ToTensor()\n",
    "                    ])\n",
    "test_ds = PointCloudData(X_test, Y_test, transform=None)\n",
    "print(test_ds)\n",
    "test_loader = DataLoader(dataset=test_ds, batch_size=1, shuffle=False, drop_last=False)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbdcb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nimport json\\npointnet = PointNet()\\n# Загрузка сохраненных весов модели\\npointnet.load_state_dict(torch.load(\".\\\\..\\\\models\\\\pointnet_weights_firstNetwork_FV.pth\"))\\n\\nwith open(\"../../Data/dumped/test_itoname_test_1.json\") as f:\\n    itoname_test = json.load(f)\\n\\npointnet.eval().to(\\'cpu\\')\\nall_preds = []\\nall_labels = []\\n# Создание списка с именами файлов\\nfilenames = []\\nwith torch.no_grad():\\n    for i, data in enumerate(test_loader):\\n        inputs, labels = data[\\'pointcloud\\'].float(), data[\\'category\\']\\n        outputs, __, __ = pointnet(inputs) #inputs.transpose(1,2)\\n        _, preds = torch.max(outputs.data, 1)\\n        all_preds += list(preds.numpy())\\n        all_labels += list(labels.numpy())\\n        filenames.append(itoname_test[str(i)])\\n\\n# Сохранение в файл Excel\\n#results_df.to_excel(\\'C:\\\\Users\\\\acer\\\\фв\\\\patient_classes.xlsx\\', index=False)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "import json\n",
    "pointnet = PointNet()\n",
    "# Загрузка сохраненных весов модели\n",
    "pointnet.load_state_dict(torch.load(\".\\\\..\\\\models\\\\pointnet_weights_firstNetwork_FV.pth\"))\n",
    "\n",
    "with open(\"../../Data/dumped/test_itoname_test_1.json\") as f:\n",
    "    itoname_test = json.load(f)\n",
    "\n",
    "pointnet.eval().to('cpu')\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "# Создание списка с именами файлов\n",
    "filenames = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs, labels = data['pointcloud'].float(), data['category']\n",
    "        outputs, __, __ = pointnet(inputs) #inputs.transpose(1,2)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        all_preds += list(preds.numpy())\n",
    "        all_labels += list(labels.numpy())\n",
    "        filenames.append(itoname_test[str(i)])\n",
    "\n",
    "# Сохранение в файл Excel\n",
    "#results_df.to_excel('C:\\\\Users\\\\acer\\\\фв\\\\patient_classes.xlsx', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66008b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "results_df = pd.DataFrame({\n",
    "    'Filename': filenames,\n",
    "    'Class': all_preds,\n",
    "    \"Labels\": all_labels\n",
    "})\n",
    "\n",
    "# Сохранение в файл Excel\n",
    "results_df.to_csv('./train_2_test_1.csv', index=False)\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Инициализация модели\n",
    "pointnet = PointNet()\n",
    "# Загрузка сохраненных весов модели\n",
    "pointnet.load_state_dict(torch.load(\".\\\\..\\\\models\\\\pointnet_weights_firstNetwork_FV.pth\"))\n",
    "with open(\"../../Data/dumped/test_itoname_test_1.json\") as f:\n",
    "    itoname_test = json.load(f)\n",
    "\n",
    "pointnet.eval().to('cpu')\n",
    "all_probs = []  # Список для вероятностей\n",
    "all_labels = []  # Список для истинных меток\n",
    "\n",
    "# Создание списка с именами файлов\n",
    "filenames = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs, labels = data['pointcloud'].float(), data['category']\n",
    "        outputs, _, = pointnet(inputs)  # inputs.transpose(1,2) если нужно\n",
    "       \n",
    "        # Извлечение вероятностей для класса 1\n",
    "        probs = outputs[:, 1]\n",
    "        \n",
    "        all_probs += list(probs.numpy())  # Сохраним вероятности\n",
    "        all_labels += list(labels.numpy())\n",
    "        filenames.append(itoname_test[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5605c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Filename': filenames,\n",
    "    'Probability_Class_1': all_probs,  # Вероятности для класса 1\n",
    "    'Labels': all_labels  # Истинные метки\n",
    "})\n",
    "\n",
    "# Сохранение в файл CSV\n",
    "results_df.to_csv('./train_2_test_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4826cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score\n",
    "from clearml import Task, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b90999",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "classes=['здоров','болен']\n",
    "plt.figure(figsize=(5,5))\n",
    "plot_confusion_matrix(cm, classes, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad076e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
